import re
import csv
import gzip
import os
import requests
import zipfile
import io
import pandas as pd
import urllib3
import chardet
import shutil
from datetime import datetime, timedelta

# Suppress the InsecureRequestWarning
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EdinetReportDownloader:

    def __init__(self):
        self.use_cache = True
        # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å°‚ç”¨
        self.incremental_cache_file = "incremental_doc_indexes.tsv.gz"

        self.meta_begin_x_year_ago = 10
        self.meta_end_x_year_ago = 0

        self.begin_x_year_ago = 10 #å®Ÿéš›ã«ã¯éå»10å¹´ã—ã‹å–å¾—ã§ããªã„ãŒã€10å¹´ã ã¨ä¸€éƒ¨æŠœã‘ã‚‹æ›¸é¡ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ãƒãƒƒãƒ•ã‚¡ãƒ¼ã¨ã—ã¦1å¹´
        self.end_x_year_ago = 0
        
        # æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸ã¨å››åŠæœŸå ±å‘Šæ›¸ã®å–å¾—æœŸé–“ï¼ˆå¹´ï¼‰
        self.recent_docs_years = 10

        #self.begin_x_year_ago = 0.5 #éå»13å¹´åˆ†ã‚’è¾¿ã‚‹
        self.is_debug = True
        
        # EDINETã®æ›¸é¡ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰
        self.DOC_TYPE_CODE_SECURITIES_REGISTRATION = '030'  # æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸
        self.DOC_TYPE_CODE_SECURITIES_REPORT = '120'  # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸
        self.DOC_TYPE_CODE_QUARTERLY_REPORT = '140'  # å››åŠæœŸå ±å‘Šæ›¸
        
        #self.current_dir = os.path.dirname(os.path.abspath(__file__))
        #print(self.current_dir)

        self.API_KEY = os.environ['EDINET_API_KEY'].rstrip()
        self.BASE_URL = "https://api.edinet-fsa.go.jp/api/v2/documents"
        self.edinet_url = "https://disclosure2dl.edinet-fsa.go.jp/searchdocument/codelist/Edinetcode.zip"

        self.OUTPUT_DIR = os.path.join('data', 'output', 'edinet_db')
        self.REPORTS_DIR = os.path.join(self.OUTPUT_DIR, 'ipo_reports')
        self.NEW_REPORTS_DIR = os.path.join(self.OUTPUT_DIR, 'ipo_reports_new')
        self.EDINET_CODE_DIR = os.path.join(self.OUTPUT_DIR, 'edinet_codes')
        self.edinet_codes_csv_path = os.path.join(self.EDINET_CODE_DIR, "EdinetcodeDlInfo.csv")
        self.edinet_codes_zip_path = os.path.join(self.EDINET_CODE_DIR, "Edinetcode.zip")

        if not os.path.exists(self.OUTPUT_DIR):
            os.makedirs(self.OUTPUT_DIR)
        if not os.path.exists(self.REPORTS_DIR):
            os.mkdir(self.REPORTS_DIR)
        if not os.path.exists(self.EDINET_CODE_DIR):
            os.mkdir(self.EDINET_CODE_DIR)
        if not os.path.exists(self.EDINET_CODE_DIR):
            os.makedirs(self.EDINET_CODE_DIR)


    def download_and_extract_edinet_zip(self):

        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = requests.get(self.edinet_url)
        with open(self.edinet_codes_zip_path, "wb") as file:
            file.write(response.content)

        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹
        with zipfile.ZipFile(self.edinet_codes_zip_path, 'r') as zip_ref:
            zip_ref.extractall(self.EDINET_CODE_DIR)

        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        os.remove(self.edinet_codes_zip_path)

        #print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã¯ {self.EDINET_CODE_DIR} ã«å±•é–‹ã•ã‚Œã¾ã—ãŸã€‚")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã€Œè¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã€ã‚’ã‚­ãƒ¼ã«ã€Œï¼¥ï¼¤ï¼©ï¼®ï¼¥ï¼´ã‚³ãƒ¼ãƒ‰ã€ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
        code5_to_edinet_dict = {}
        edinet_to_company_dict = {}
        #with open(self.edinet_codes_csv_path, "r", encoding='shift_jis') as csvfile:
        with open(self.edinet_codes_csv_path, "r", encoding='cp932') as csvfile:
            reader = csv.reader(csvfile)
            next(reader)  # æœ€åˆã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            header = next(reader)  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’èª­ã¿è¾¼ã‚€
            for row in reader:
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã«åŸºã¥ã„ã¦è¡Œã‚’è¾æ›¸ã¨ã—ã¦è§£æ
                row_dict = dict(zip(header, row))
                edinet_code = row_dict['ï¼¥ï¼¤ï¼©ï¼®ï¼¥ï¼´ã‚³ãƒ¼ãƒ‰']
                company_code = row_dict['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰']
                company_name = row_dict['æå‡ºè€…å']
                if len(company_code) == 0:
                    # éä¸Šå ´ã‹ä¸Šå ´å»ƒæ­¢ã®æ™‚ã«è©²å½“ã™ã‚‹
                    continue

                code5_to_edinet_dict[company_code] = {
                    'edinet_code': edinet_code,
                    'company_name': company_name
                }
                edinet_to_company_dict[edinet_code] = {
                    'company_code': company_code, #5æ¡
                    'company_name': company_name
                }

        #print(code5_to_edinet_dict)
        #print(edinet_to_company_dict)
        return code5_to_edinet_dict, edinet_to_company_dict

    def get_company_dict(self):
        code5_to_edinet_dict, edinet_to_company_dict = self.download_and_extract_edinet_zip()
        return edinet_to_company_dict

    def get_ipo_company_dict(self, companies_list):
        code5_to_edinet_dict, edinet_to_company_dict = self.download_and_extract_edinet_zip()
        ipo_edinet_to_company_dict = {}
        for companies in companies_list:
            for index, company in enumerate(companies):
                company_code4, company_name = company
                company_code5 = company_code4 + '0'
                if company_code5 in code5_to_edinet_dict:
                    edinet_info = code5_to_edinet_dict[company_code5]
                    edinet_code = edinet_info['edinet_code']
                    #company_code = edinet_info['company_code']
                    ipo_edinet_to_company_dict[edinet_code] = {
                        'company_code': company_code5,
                        'company_name': company_name
                    }
        return ipo_edinet_to_company_dict

    def request_doc_json(self, date):
        params = {
        "type" : 2,
        "date" : date,
        "Subscription-Key": self.API_KEY
        }
        try:
            res = requests.get(self.BASE_URL + ".json", params=params, verify=False)
            res.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèª
            json_data = res.json()
            #if self.is_debug:
            #    print(f"DEBUG: json_data: {json_data}")

            if 'results' in json_data:
                return json_data['results']
            elif 'metadata' in json_data and json_data['metadata'].get('status') == '404':
                raise ValueError("Error 404: The requested resource was not found.")
            else:
                raise KeyError("The key 'results' was not found in the response.")
        except requests.exceptions.HTTPError as http_err:
            print(f"HTTP error occurred: {http_err}")
        except requests.exceptions.ConnectionError as conn_err:
            print(f"Connection error occurred: {conn_err}")
        except requests.exceptions.Timeout as timeout_err:
            print(f"Timeout error occurred: {timeout_err}")
        except requests.exceptions.RequestException as req_err:
            print(f"An error occurred: {req_err}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
        return []

    def request_doc_elements(self, date, doc_id, doc_name):
        params = {
            "type" : 5, #csv
            "date" : date,
            "Subscription-Key": self.API_KEY
        }
        try:
            response = requests.get(f"{self.BASE_URL}/{doc_id}", params=params, verify=False)
            response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€ä¾‹å¤–ãŒã‚¹ãƒ­ãƒ¼ã•ã‚Œã¾ã™
            if 'application/octet-stream' not in response.headers.get('Content-Type', ''):
                content_type = response.headers.get('Content-Type', 'Unknown')
                print(f"Error: Response is not a ZIP file (octet-stream). Content-Type: {content_type}.")
                print(f"Response: {response.json()}")
                #print(f"Response content1: {response.content[:1000]}")  # æœ€åˆã®1000ãƒã‚¤ãƒˆã®ã¿å‡ºåŠ›ã—ã¦å†…å®¹ã‚’ç¢ºèª
                return []
            #tsv_df = self.extract_tsv_from_zip(response)
            tsv_raw_data = self.extract_tsv_from_zip(response)
        except requests.exceptions.RequestException as e:
            print(f"Error fetching document elements for doc_id {doc_id} on {date}: {e}")
            return []
        return tsv_raw_data


    def extract_tsv_from_zip(self, response):

        with zipfile.ZipFile(io.BytesIO(response.content)) as the_zip:
            for file_name in the_zip.namelist():
                #print(f"file_name: {file_name}")
                if file_name.startswith('XBRL_TO_CSV/jpcrp') and file_name.endswith('.csv'):
                    with the_zip.open(file_name) as the_file:
                        raw_data = the_file.read()
                        return raw_data

    def save_securities_reports_in_one_day(self, date_str, edinet_to_company_dict):
        doc_metas = self.request_doc_json(date_str)
        tsv_rows = []
        for meta in doc_metas:
            edinet_code = meta['edinetCode']
            if not edinet_code in edinet_to_company_dict:
                continue # ä¸Šå ´å»ƒæ­¢ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—

            if meta['csvFlag'] == '1' and (
                meta['docTypeCode'] == self.DOC_TYPE_CODE_SECURITIES_REPORT or 
                meta['docTypeCode'] == self.DOC_TYPE_CODE_SECURITIES_REGISTRATION or
                meta['docTypeCode'] == self.DOC_TYPE_CODE_QUARTERLY_REPORT):
                if self.is_debug:
                    print(f"name = {meta['filerName']}, docDescription = {meta['docDescription']}, docId = {meta['docID']}")
                tsv_rows.append([date_str, edinet_code, meta['docTypeCode'], meta['docID']])
        
        return tsv_rows

    def save_securities_docs(self, company_doc_info, doc_type_code, company_folder, doc_name, company_code4, company_name, start_date=None, end_date=None):
        sorted_documents = company_doc_info[company_doc_info['docTypeCode'] == doc_type_code].sort_values('date', ascending=True)

        if sorted_documents.empty:
            print(f"\033[91mError: No documents found for {company_name} ({company_code4}) on doc_type_code = {doc_type_code}\033[0m")
            return

        # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®å‡¦ç†
        if start_date is None or end_date is None:
            # å¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼šæœ€ã‚‚å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ—¥ä»˜ã‹ã‚‰è¨ˆç®—
            oldest_date = sorted_documents.iloc[0]['date']
            calc_start_date = datetime.strptime(oldest_date, '%Y-%m-%d')
            calc_end_date = calc_start_date + timedelta(days=(self.begin_x_year_ago - self.end_x_year_ago) * 365)
            
            # æ–‡å­—åˆ—å½¢å¼ã«å¤‰æ›
            start_date_str = calc_start_date.strftime('%Y-%m-%d')
            end_date_str = calc_end_date.strftime('%Y-%m-%d')
        else:
            # å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã‚’ä½¿ç”¨
            start_date_str = start_date.strftime('%Y-%m-%d') if isinstance(start_date, datetime) else start_date
            end_date_str = end_date.strftime('%Y-%m-%d') if isinstance(end_date, datetime) else end_date

        # æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_documents = sorted_documents[
            (start_date_str <= sorted_documents['date']) &
            (sorted_documents['date'] <= end_date_str)
        ]

        os.makedirs(company_folder, exist_ok=True)

        for i in range(len(filtered_documents)):
            document = filtered_documents.iloc[i]
            tsv_raw_data = self.request_doc_elements(document['date'], document['docID'], doc_name)
            if tsv_raw_data:
                file_path = f"{company_folder}/{document['date']}_{doc_name}.tsv"
                with open(file_path, 'wb') as debug_file:
                    debug_file.write(tsv_raw_data)
                print(f"Saved: {file_path}")
            else:
                print(f"\033[91mError: No valid '{doc_name}' found for {company_name} ({company_code4}) on {document['date']}\033[0m")




    def get_last_cached_date(self, cache_path):
        """
        æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æœ€æ–°ã®æ—¥ä»˜ã‚’å–å¾—
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯æŒ‡å®šå¹´æ•°å‰ã®æ—¥ä»˜ã‚’è¿”ã™
        """
        if not os.path.exists(cache_path):
            # åˆå›å®Ÿè¡Œæ™‚ã¯æŒ‡å®šå¹´æ•°å‰ã‹ã‚‰é–‹å§‹
            return datetime.now() - timedelta(days=self.meta_begin_x_year_ago * 365)
        
        try:
            with gzip.open(cache_path, 'rt', encoding='utf-8') as f:
                existing_data = pd.read_csv(f, sep='\t', dtype={'date': str})
            
            if existing_data.empty:
                return datetime.now() - timedelta(days=self.meta_begin_x_year_ago * 365)
            
            # æœ€æ–°ã®æ—¥ä»˜ã‚’å–å¾—
            last_date_str = existing_data['date'].max()
            return datetime.strptime(last_date_str, '%Y-%m-%d')
            
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return datetime.now() - timedelta(days=self.meta_begin_x_year_ago * 365)

    def get_missing_date_range(self, cache_path):
        """
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªæ—¥ä»˜ç¯„å›²ã‚’è¨ˆç®—
        """
        last_cached_date = self.get_last_cached_date(cache_path)
        current_date = datetime.now()
        
        # æœ€æ–°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¿Œæ—¥ã‹ã‚‰ç¾åœ¨æ—¥ä»˜ã¾ã§
        start_date = last_cached_date + timedelta(days=1)
        end_date = current_date
        
        return start_date, end_date, last_cached_date

    def download_incremental_data(self, start_date, end_date, companies_dict):
        """
        æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²ã®ã¿EDINETã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        """
        if self.is_debug:
            print(f"ğŸ“¥ å·®åˆ†ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')}")
        
        new_data = []
        current_date = start_date
        
        while current_date <= end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            if self.is_debug:
                print(f"  å‡¦ç†ä¸­: {date_str}")
            
            # æ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦EDINET APIã‚’å‘¼ã³å‡ºã—
            day_data = self.save_securities_reports_in_one_day(date_str, companies_dict)
            new_data.extend(day_data)
            
            current_date += timedelta(days=1)
        
        return pd.DataFrame(new_data, columns=['date', 'edinet_code', 'docTypeCode', 'docID'])

    def merge_and_save_cache(self, new_data, cache_path):
        """
        æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦ä¿å­˜
        """
        if os.path.exists(cache_path):
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            with gzip.open(cache_path, 'rt', encoding='utf-8') as f:
                existing_data = pd.read_csv(f, sep='\t', dtype={'date': str, 'edinet_code': str, 'docTypeCode': str, 'docID': str})
            
            # æ–°æ—§ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            combined_data = pd.concat([existing_data, new_data], ignore_index=True)
            if self.is_debug:
                print(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data)}ä»¶ + æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(new_data)}ä»¶")
        else:
            combined_data = new_data
            if self.is_debug:
                print(f"ğŸ“Š åˆå›ä½œæˆ: {len(new_data)}ä»¶")
        
        # é‡è¤‡å‰Šé™¤ï¼ˆåŒã˜æ—¥ä»˜ãƒ»ä¼æ¥­ãƒ»æ–‡æ›¸ç¨®åˆ¥ã®é‡è¤‡ã‚’é™¤å»ï¼‰
        combined_data = combined_data.drop_duplicates(
            subset=['date', 'edinet_code', 'docTypeCode'], 
            keep='last'
        )
        
        # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆ
        combined_data = combined_data.sort_values('date')
        
        # åœ§ç¸®ä¿å­˜
        with gzip.open(cache_path, 'wt', encoding='utf-8') as f:
            combined_data.to_csv(f, sep='\t', index=False)
        
        if self.is_debug:
            print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {len(combined_data)}ä»¶ â†’ {cache_path}")
        return combined_data

    def run_incremental_update(self, cache_path, companies_dict):
        """
        ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
        """
        if self.is_debug:
            print("ğŸ”„ ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°é–‹å§‹")
        
        # 1. ä¸è¶³ã—ã¦ã„ã‚‹æ—¥ä»˜ç¯„å›²ã‚’è¨ˆç®—
        start_date, end_date, last_cached_date = self.get_missing_date_range(cache_path)
        
        if start_date > end_date:
            if self.is_debug:
                print("âœ… æ›´æ–°ä¸è¦: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯æœ€æ–°çŠ¶æ…‹ã§ã™")
            return
        
        if self.is_debug:
            print(f"ğŸ“… å‰å›ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€æ–°æ—¥: {last_cached_date.strftime('%Y-%m-%d')}")
            print(f"ğŸ†• æ›´æ–°å¯¾è±¡æœŸé–“: {(end_date - start_date).days + 1}æ—¥é–“")
        
        # 2. å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        new_data = self.download_incremental_data(start_date, end_date, companies_dict)
        
        if new_data.empty:
            if self.is_debug:
                print("âš ï¸  æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # 3. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆã—ã¦ä¿å­˜
        self.merge_and_save_cache(new_data, cache_path)
        
        if self.is_debug:
            print("âœ… ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å®Œäº†")

    def validate_incremental_update(self, cache_path):
        """
        ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯
        """
        try:
            with gzip.open(cache_path, 'rt', encoding='utf-8') as f:
                data = pd.read_csv(f, sep='\t', dtype={'date': str})
            
            # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if data.empty:
                return False, "ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™"
            
            # æ—¥ä»˜ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            dates = sorted(data['date'].unique())
            if len(dates) < 2:
                return True, "ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—"
            
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
            current_date = datetime.now().strftime('%Y-%m-%d')
            recent_dates = [d for d in dates if d >= (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')]
            
            if not recent_dates:
                return False, "ç›´è¿‘7æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
            
            return True, f"æ¤œè¨¼æˆåŠŸ: {len(data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿"
            
        except Exception as e:
            return False, f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}"


    def save_securities_reports(self, companies_list = None, skip_json = True):
        tracking_days = int(365 * self.meta_begin_x_year_ago)
        if companies_list:
            edinet_to_company_dict = self.get_ipo_company_dict(companies_list)
        else:
            edinet_to_company_dict = self.get_company_dict()

        # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å°‚ç”¨
        doc_meta_path = os.path.join(self.EDINET_CODE_DIR, self.incremental_cache_file)
        print(f"DEBUG: ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ãƒ¢ãƒ¼ãƒ‰ - cache_path: {doc_meta_path}")
        
        # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å®Ÿè¡Œ
        self.run_incremental_update(doc_meta_path, edinet_to_company_dict)
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        is_valid, message = self.validate_incremental_update(doc_meta_path)
        if not is_valid:
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼: {message}")
            raise RuntimeError(f"ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {message}")
        
        if self.is_debug:
            print(f"âœ… {message}")

        # Load the single TSV file
        with gzip.open(doc_meta_path, 'rt', encoding='utf-8') as f:
            full_data = pd.read_csv(f, sep='\t', dtype={'date': str, 'edinet_code': str, 'docTypeCode': str, 'docID': str})

        # Process each company to get the latest reports
        for edinet_code, company_doc_info in full_data.groupby('edinet_code'):
            if edinet_code not in edinet_to_company_dict:
                continue  # è¾æ›¸ã«å­˜åœ¨ã—ãªã„edinet_codeã¯ã‚¹ã‚­ãƒƒãƒ—
            company = edinet_to_company_dict[edinet_code]
            company_code5 = company['company_code']
            company_name = company['company_name']
            company_code4 = company_code5[:-1]
            if self.is_debug:
                print(f"DEBUG: code = {company_code4}, name = {company_name}")
            
            # æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸ã¨å››åŠæœŸå ±å‘Šæ›¸ã¯éå»2å¹´é–“ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—
            current_date = datetime.now()
            two_years_ago = current_date - timedelta(days=self.recent_docs_years * 365)
            
            # æ—¥ä»˜æ–‡å­—åˆ—ã«å¤‰æ›
            start_date_str = two_years_ago.strftime('%Y-%m-%d')
            end_date_str = current_date.strftime('%Y-%m-%d')

            # æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸ã‚’ä¿å­˜ï¼ˆäº‹æ¥­ã®å†…å®¹ãŒæ›¸ã‹ã‚Œã¦ãªã„å ´åˆãŒå¤šã„ã‹ã‚‚ï¼Ÿï¼Ÿï¼‰
            company_folder = f"{self.NEW_REPORTS_DIR}/{company_code4}_{company_name}/securities_registration_statement"
            self.save_securities_docs(company_doc_info, self.DOC_TYPE_CODE_SECURITIES_REGISTRATION, company_folder, 'æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸', company_code4, company_name, start_date_str, end_date_str)
            
            # å››åŠæœŸå ±å‘Šæ›¸ã‚’ä¿å­˜
            #company_folder = f"{self.NEW_REPORTS_DIR}/{company_code4}_{company_name}/quarterly_reports"
            #self.save_securities_docs(company_doc_info, self.DOC_TYPE_CODE_QUARTERLY_REPORT, company_folder, 'å››åŠæœŸå ±å‘Šæ›¸', company_code4, company_name, start_date_str, end_date_str)

            # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚’ä¿å­˜
            #company_folder = f"{self.REPORTS_DIR}/{company_code4}_{company_name}/annual_securities_reports"
            #self.save_securities_docs(company_doc_info, self.DOC_TYPE_CODE_SECURITIES_REPORT, company_folder, 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸', company_code4, company_name)

    def run(self):
        self.save_securities_reports()
        self.save_companies_info_list()
    
    def save_companies_info_list(self):
        pass

    def show_cache_status(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çŠ¶æ…‹ã‚’è¡¨ç¤º"""
        print("ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹:")
        
        # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çŠ¶æ…‹
        incremental_cache = os.path.join(self.EDINET_CODE_DIR, self.incremental_cache_file)
        if os.path.exists(incremental_cache):
            mtime = datetime.fromtimestamp(os.path.getmtime(incremental_cache))
            print(f"  ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥: å­˜åœ¨ (æ›´æ–°æ—¥æ™‚: {mtime.strftime('%Y-%m-%d %H:%M:%S')})")
            
            try:
                last_cached_date = self.get_last_cached_date(incremental_cache)
                print(f"  æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {last_cached_date.strftime('%Y-%m-%d')}")
                
                # æ¬¡å›æ›´æ–°ã§å–å¾—ã•ã‚Œã‚‹æ—¥ä»˜ç¯„å›²ã‚’è¡¨ç¤º
                start_date, end_date, _ = self.get_missing_date_range(incremental_cache)
                if start_date <= end_date:
                    days_count = (end_date - start_date).days + 1
                    print(f"  æ¬¡å›æ›´æ–°å¯¾è±¡: {start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')} ({days_count}æ—¥é–“)")
                else:
                    print("  æ¬¡å›æ›´æ–°å¯¾è±¡: ãªã—ï¼ˆæœ€æ–°çŠ¶æ…‹ï¼‰")
            except Exception as e:
                print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥è§£æã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print("  ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥: å­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆåˆå›å®Ÿè¡Œæ™‚ã«ä½œæˆï¼‰")



    # æ—¥ä»˜ã‚’æŠ½å‡ºã—ã¦ã‚½ãƒ¼ãƒˆ
    def extract_date(self, file_path):
        file_name = os.path.basename(file_path)
        match = re.match(r'(\d{4}-\d{2}-\d{2})_', file_name)
        if match:
            return datetime.strptime(match.group(1), '%Y-%m-%d')
        return None

    def find_report_path(self, code, latest=False):
        # codeã«è©²å½“ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦‹ã¤ã‘ã‚‹
        code_folder_pattern = f"{code}_.*"
        code_folders = [f for f in os.listdir(self.REPORTS_DIR) if os.path.isdir(os.path.join(self.REPORTS_DIR, f)) and re.match(code_folder_pattern, f)]
        
        all_files = []

        for folder in code_folders:
            securities_path = os.path.join(self.REPORTS_DIR, folder, 'securities_registration_statement')
            annual_path = os.path.join(self.REPORTS_DIR, folder, 'annual_securities_reports')

            # securities_registration_statementãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒã‚§ãƒƒã‚¯
            if os.path.exists(securities_path):
                for file in os.listdir(securities_path):
                    if file.endswith('.tsv'):
                        all_files.append(os.path.join(securities_path, file))

            # annual_securities_reportsãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒã‚§ãƒƒã‚¯
            if os.path.exists(annual_path):
                for file in os.listdir(annual_path):
                    if file.endswith('.tsv'):
                        all_files.append(os.path.join(annual_path, file))

        if not all_files:
            return None

        all_files.sort(key=lambda x: self.extract_date(x), reverse=latest)
        return all_files[0] if all_files else None



if __name__ == "__main__":
    tsv = EdinetReportDownloader()
    tsv.run()

    #print(report_dict)
    #tsv.save_ipo_securities_reports(companies = {
    #    'E38948': {'company_code': '55880', 'company_name': 'ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¢ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°'}
    #})
