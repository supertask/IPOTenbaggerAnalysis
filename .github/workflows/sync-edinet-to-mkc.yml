name: Sync EDINET IPO reports to MKC

on:
#  schedule:
#    # 毎月1日 09:00 JST（GitHubはUTC基準なので 00:00 UTC）
#    - cron: '0 0 1 * *'
  push:
    tags:
      - 'center-*'        # 例: center-2025-08-31
  workflow_dispatch:

concurrency:
  group: sync-edinet-to-mkc
  cancel-in-progress: false

jobs:
  sync:
    runs-on: ubuntu-latest
    permissions:
      contents: read   # IPOTenbaggerAnalysis の読み取りのみ。書き込みはPATで実施
    steps:
      - name: Free up disk space
        run: |
          echo "=== Before cleanup ==="
          df -h
          
          # Remove unnecessary software to free up space (more aggressive)
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf /usr/local/graalvm/
          sudo rm -rf /usr/local/.ghcup/
          sudo rm -rf /usr/local/share/powershell
          sudo rm -rf /usr/local/share/chromium
          sudo rm -rf /usr/local/lib/node_modules
          sudo rm -rf /opt/pipx_bin
          sudo rm -rf /opt/microsoft
          sudo rm -rf /usr/share/swift
          sudo rm -rf /usr/local/share/vcpkg
          sudo rm -rf /usr/local/julia*
          sudo rm -rf /usr/share/miniconda
          sudo rm -rf /usr/local/aws-cli
          
          # Clean system caches and temporary files
          sudo apt-get clean
          sudo apt-get autoremove -y
          sudo journalctl --vacuum-time=1d
          sudo find /tmp -type f -exec rm -f {} + 2>/dev/null || true
          sudo find /var/tmp -type f -exec rm -f {} + 2>/dev/null || true
          sudo find /var/cache -type f -exec rm -f {} + 2>/dev/null || true
          
          # Clear Docker if present
          docker system prune -af 2>/dev/null || true
          docker volume prune -f 2>/dev/null || true
          
          echo "=== After cleanup ==="
          df -h
      - name: Checkout IPOTenbaggerAnalysis (sparse)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          # 対象ディレクトリのみ取得して高速化
          sparse-checkout: |
            data/output/
          sparse-checkout-cone: true

      - name: Ensure secret exists
        run: |
          if [ -z "${{ secrets.MILLIONARE_KNOWLEDGE_CENTER_TOKEN }}" ]; then
            echo "::error::Secret MILLIONARE_KNOWLEDGE_CENTER_TOKEN is not set in IPOTenbaggerAnalysis (Settings > Secrets and variables > Actions)."
            exit 1
          fi

      - name: Checkout MillionareKnowledgeCenter
        uses: actions/checkout@v4
        with:
          repository: supertask/MillionareKnowledgeCenter
          ref: main                           # 宛先のデフォルトブランチに合わせて必要なら変更
          token: ${{ secrets.MILLIONARE_KNOWLEDGE_CENTER_TOKEN }}
          path: mkc
          fetch-depth: 0

      - name: Sync ipo_reports -> knowledge/edinet_db
        shell: bash
        run: |
          set -euo pipefail
          SRC_BASE="data/output"
          DEST_BASE="mkc/knowledge/listed_companies_data"

          if [ ! -d "${SRC_BASE}" ]; then
            echo "Source base '${SRC_BASE}' not found. Nothing to sync."
            exit 0
          fi

          # Git configuration (共通設定)
          cd mkc
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          cd ..

          # Function to commit and push changes
          commit_and_push() {
            local commit_msg="$1"
            local file_count="$2"
            
            echo "=== Committing and pushing $file_count files ==="
            cd mkc
            
            # Clean up unwanted files
            find knowledge/listed_companies_data -name '.DS_Store' -type f -delete 2>/dev/null || true
            find knowledge/listed_companies_data -name '*.tmp' -type f -delete 2>/dev/null || true
            find knowledge/listed_companies_data -name '*.temp' -type f -delete 2>/dev/null || true

            git add -A
            if git diff --staged --quiet; then
              echo "No changes to commit."
              cd ..
              return 0
            fi

            git commit -m "$commit_msg"
            git push origin HEAD
            echo "✅ Successfully pushed batch with $file_count files"
            cd ..
            return 0
          }
          
          # Function to handle large directories by processing subdirectories in batches
          sync_large_directory() {
            local src_dir="$1"
            local dest_dir="$2"
            local cat_name="$3"
            
            echo "🔄 LARGE DIRECTORY DETECTED: Processing ${cat_name} using subdirectory batching"
            
            # Create destination directory
            mkdir -p "${dest_dir}"
            
            # Get list of subdirectories
            shopt -s nullglob
            local subdirs=("${src_dir}"/*/)
            
            if [ ${#subdirs[@]} -eq 0 ]; then
              echo "  No subdirectories found in ${src_dir}. Processing as single directory."
              # Process files in the root of the large directory
              rsync -avz --compress-level=6 --partial --inplace \
                --delete --delete-excluded \
                --exclude='.git/' --exclude='.DS_Store' --exclude='*.tmp' --exclude='*.temp' \
                --timeout=300 \
                "${src_dir}/" "${dest_dir}/" || return 1
              return 0
            fi
            
            echo "  Found ${#subdirs[@]} subdirectories to process"
            
            # Process subdirectories in optimized batches
            local subdir_batch_size=50
            local processed_subdirs=0
            
            for ((k=0; k<${#subdirs[@]}; k+=subdir_batch_size)); do
              local subdir_batch_end=$((k + subdir_batch_size))
              if [ $subdir_batch_end -gt ${#subdirs[@]} ]; then
                subdir_batch_end=${#subdirs[@]}
              fi
              
              echo "  📦 Processing subdirectory batch $((k/subdir_batch_size + 1)): items $((k+1)) to $subdir_batch_end"
              
              # Sync this batch of subdirectories
              for ((l=k; l<subdir_batch_end; l++)); do
                local subdir="${subdirs[l]}"
                local subdir_name="$(basename "${subdir%/}")"
                local dest_subdir="${dest_dir}/${subdir_name}"
                
                echo "    Syncing ${subdir_name}..."
                mkdir -p "${dest_subdir}"
                
                rsync -avz --compress-level=6 --partial --inplace \
                  --delete --delete-excluded \
                  --exclude='.git/' --exclude='.DS_Store' --exclude='*.tmp' --exclude='*.temp' \
                  --timeout=300 \
                  "${subdir}" "${dest_subdir}/" || {
                    echo "    ❌ Failed to sync ${subdir_name}"
                    return 1
                  }
                
                echo "    ✓ Successfully synced ${subdir_name}"
                
                # Immediately delete the source subdirectory to save space
                rm -rf "${subdir}" 2>/dev/null || true
                echo "    🗑️  Cleaned up source: ${subdir}"
                
                ((processed_subdirs++))
              done
              
              # Check disk space after each batch
              available_space=$(df . | tail -1 | awk '{print $4}')
              echo "    Disk space after batch: ${available_space} KB available"
              
              if [ "$available_space" -lt 2000000 ]; then
                echo "    ⚠️  Running preventive cleanup..."
                sudo apt-get clean
                docker system prune -f 2>/dev/null || true
                find /tmp -type f -delete 2>/dev/null || true
                sync
                echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null
              fi
            done
            
            echo "  ✅ Completed processing ${cat_name}: ${processed_subdirs} subdirectories synced"
            
            # Clean up any remaining files in the source directory
            if [ -d "${src_dir}" ]; then
              rm -rf "${src_dir}" 2>/dev/null || true
              echo "  🗑️  Final cleanup: removed ${src_dir}"
            fi
            
            return 0
          }

          shopt -s nullglob
          # 直下の「フォルダ群」を列挙（ファイルは無視）
          dirs=("${SRC_BASE}"/*/)
          if [ ${#dirs[@]} -eq 0 ]; then
            echo "No folders under ${SRC_BASE}."
            exit 0
          fi

          echo "Total directories to sync: ${#dirs[@]}"
          
          # Pre-check directory sizes and warn about large directories
          echo "=== Pre-scan: Directory size analysis ==="
          large_dirs_count=0
          total_estimated_files=0
          for dir in "${dirs[@]}"; do
            dir_file_count=$(find "${dir}" -type f 2>/dev/null | wc -l || echo 0)
            total_estimated_files=$((total_estimated_files + dir_file_count))
            if [ "$dir_file_count" -gt 10000 ]; then
              echo "🔄 LARGE DIRECTORY DETECTED: $(basename "${dir%/}") - ${dir_file_count} files (will use subdirectory batching)"
              ((large_dirs_count++))
            elif [ "$dir_file_count" -gt 1000 ]; then
              echo "📁 Large directory: $(basename "${dir%/}") - ${dir_file_count} files"
            else
              echo "📄 Directory: $(basename "${dir%/}") - ${dir_file_count} files"
            fi
          done
          
          echo "=== Pre-scan Summary ==="
          echo "Total estimated files: ${total_estimated_files}"
          echo "Large directories (>10k files) to process with subdirectory batching: ${large_dirs_count}"
          
          echo "=== Disk space before sync ==="
          df -h

          # Process directories in smaller batches to manage disk usage
          # Use extremely small batch size to be ultra-conservative with disk space
          batch_size=5
          total_success=0
          total_failed=0
          
          # File counting for split commits
          files_in_current_batch=0
          max_files_per_push=300
          push_count=1
          
          # Disable immediate exit on error for the sync loop to handle errors gracefully
          set +e
          
          for ((i=0; i<${#dirs[@]}; i+=batch_size)); do
            batch_end=$((i + batch_size))
            if [ $batch_end -gt ${#dirs[@]} ]; then
              batch_end=${#dirs[@]}
            fi
            
            echo "=== Processing batch $((i/batch_size + 1)): directories $((i+1)) to $batch_end ==="
            
            sync_success=0
            sync_failed=0
            
            for ((j=i; j<batch_end; j++)); do
              src_dir="${dirs[j]}"
              cat_name="$(basename "${src_dir%/}")"
              dest_dir="${DEST_BASE}/${cat_name}"
              mkdir -p "${dest_dir}"

              # Count files in source directory
              dir_file_count=$(find "${src_dir}" -type f 2>/dev/null | wc -l || echo 0)
              
              # Handle extremely large directories using subdirectory batching
              if [ "$dir_file_count" -gt 10000 ]; then
                echo "📂 LARGE DIRECTORY: ${src_dir} (${dir_file_count} files > 10,000)"
                echo "    Using subdirectory batching approach..."
                
                if sync_large_directory "${src_dir}" "${dest_dir}" "${cat_name}"; then
                  echo "✓ Successfully synced large directory: ${src_dir}"
                  ((sync_success++))
                  # Note: source directory already cleaned up by sync_large_directory function
                else
                  echo "✗ Failed to sync large directory: ${src_dir}"
                  ((sync_failed++))
                fi
                
                echo "  Completed processing large directory $(($j+1))/${#dirs[@]}: ${cat_name}"
                continue
              fi
              
              echo "Syncing ${src_dir} -> ${dest_dir} ($(($j+1))/${#dirs[@]}) - ${dir_file_count} files"
              echo "  Current batch total: $((files_in_current_batch + dir_file_count)) files"
              
              # Check if adding this directory would exceed the limit
              if [ $((files_in_current_batch + dir_file_count)) -gt $max_files_per_push ] && [ $files_in_current_batch -gt 0 ]; then
                echo "  📦 File limit approaching ($((files_in_current_batch + dir_file_count))>${max_files_per_push}). Pushing current batch..."
                
                # Commit and push current changes
                if [[ "${GITHUB_REF:-}" == refs/tags/* ]]; then
                  TAG_NAME="${GITHUB_REF#refs/tags/}"
                  commit_msg="chore(sync): EDINET IPO reports batch ${push_count} from IPOTenbaggerAnalysis (tag ${TAG_NAME})"
                else
                  commit_msg="chore(sync): EDINET IPO reports batch ${push_count} from IPOTenbaggerAnalysis ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
                fi
                
                commit_and_push "$commit_msg" "$files_in_current_batch"
                
                # Reset counters
                files_in_current_batch=0
                ((push_count++))
              fi
              
              # 最適化されたrsyncオプション：圧縮、進捗表示、効率的な転送
              echo "  About to execute rsync command..."
              rsync_exit_code=0
              rsync -avz --compress-level=6 --partial --inplace \
                --delete --delete-excluded \
                --exclude='.git/' \
                --exclude='.DS_Store' \
                --exclude='*.tmp' \
                --exclude='*.temp' \
                --timeout=300 \
                "${src_dir}/" "${dest_dir}/" || rsync_exit_code=$?
              
              echo "  rsync completed with exit code: ${rsync_exit_code}"
              
              if [ "${rsync_exit_code}" -eq 0 ]; then
                echo "✓ Successfully synced ${src_dir}"
                echo "  Destination contents: $(ls -la "${dest_dir}" 2>/dev/null | wc -l) items"
                ((sync_success++))
                # Add to file count only if sync was successful
                ((files_in_current_batch += dir_file_count))
              else
                echo "✗ Failed to sync ${src_dir} (error code: ${rsync_exit_code})"
                ((sync_failed++))
                
                              # If disk space is the issue, try to free up space and continue
              available_space=$(df . | tail -1 | awk '{print $4}')
                              if [ "$available_space" -lt 1500000 ]; then  # Less than 1.5GB
                echo "Critical: Low disk space. Attempting aggressive emergency cleanup..."
                
                # More aggressive cleanup
                sudo apt-get clean
                sudo apt-get autoremove -y
                sudo apt-get autoclean
                
                # Docker cleanup
                docker system prune -af 2>/dev/null || true
                docker volume prune -f 2>/dev/null || true
                docker builder prune -af 2>/dev/null || true
                docker network prune -f 2>/dev/null || true
                
                # System cleanup
                find /tmp -type f -delete 2>/dev/null || true
                find /var/tmp -type f -delete 2>/dev/null || true
                find /var/cache -type f -delete 2>/dev/null || true
                find /var/log -name "*.log" -type f -delete 2>/dev/null || true
                sudo journalctl --vacuum-time=12h
                
                # Force aggressive memory cleanup
                sync
                echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null
                
                # Clean package caches
                sudo rm -rf /var/cache/apt/archives/* 2>/dev/null || true
                sudo rm -rf ~/.cache/* 2>/dev/null || true
                
                # Check if we have enough space to continue
                available_space=$(df . | tail -1 | awk '{print $4}')
                if [ "$available_space" -lt 1000000 ]; then  # Less than 1GB
                  echo "Error: Still insufficient disk space after aggressive cleanup. Cannot continue."
                  exit 1
                fi
                
                echo "Emergency cleanup completed. Available space: ${available_space} KB"
              fi
              fi
              
              echo "  Completed processing directory $(($j+1))/${#dirs[@]}: ${cat_name} (batch total: ${files_in_current_batch} files)"
              
              # Clean up source directory after successful sync to save space
              if [ "${rsync_exit_code}" -eq 0 ] && [ -d "${src_dir}" ]; then
                echo "  Cleaning up source directory to save disk space..."
                rm -rf "${src_dir}" 2>/dev/null || true
                echo "  Source directory cleaned: ${src_dir}"
              fi
            done
            
            echo "Batch $((i/batch_size + 1)) summary: ${sync_success} successful, ${sync_failed} failed"
            
            # Update total counters
            ((total_success += sync_success))
            ((total_failed += sync_failed))
            
            # Check disk usage after each batch and clean up if needed
            echo "=== Disk usage after batch $((i/batch_size + 1)) ==="
            df -h
            
            # Clean up temporary files
            find . -name "*.tmp" -type f -delete 2>/dev/null || true
            find . -name "*.temp" -type f -delete 2>/dev/null || true
            
            # More aggressive disk monitoring and cleanup
            available_space=$(df . | tail -1 | awk '{print $4}')
            if [ "$available_space" -lt 3000000 ]; then  # Less than 3GB available
              echo "Warning: Low disk space ($available_space KB available). Running proactive cleanup..."
              sudo apt-get clean
              sudo apt-get autoremove -y
              docker system prune -af 2>/dev/null || true
              docker volume prune -f 2>/dev/null || true
              find /tmp -type f -delete 2>/dev/null || true
              find /var/tmp -type f -delete 2>/dev/null || true
              find /var/cache -type f -delete 2>/dev/null || true
              sudo journalctl --vacuum-time=1d
              sync
              echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null
              echo "Proactive cleanup completed. New available space: $(df . | tail -1 | awk '{print $4}') KB"
            fi
          done
          
          # Re-enable immediate exit on error after sync loop
          set -e
          
          # Push any remaining files
          if [ $files_in_current_batch -gt 0 ]; then
            echo "=== Pushing final batch with ${files_in_current_batch} remaining files ==="
            if [[ "${GITHUB_REF:-}" == refs/tags/* ]]; then
              TAG_NAME="${GITHUB_REF#refs/tags/}"
              commit_msg="chore(sync): EDINET IPO reports batch ${push_count} (final) from IPOTenbaggerAnalysis (tag ${TAG_NAME})"
            else
              commit_msg="chore(sync): EDINET IPO reports batch ${push_count} (final) from IPOTenbaggerAnalysis ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
            fi
            
            commit_and_push "$commit_msg" "$files_in_current_batch"
          fi

          # Final sync statistics
          echo "=== SYNC COMPLETE ==="
          echo "Total directories processed: ${#dirs[@]}"
          echo "Successfully synced: ${total_success}"
          echo "Failed to sync: ${total_failed}"
          echo "Total push batches: ${push_count}"
          
          if [ "$total_failed" -gt 0 ]; then
            echo "⚠️  Warning: Some directories failed to sync. Check logs above for details."
            if [ "$total_failed" -gt "$((${#dirs[@]} / 2))" ]; then
              echo "❌ Error: More than half of the directories failed. Exiting with error."
              exit 1
            fi
          else
            echo "✅ All directories synced successfully!"
          fi

      - name: Final cleanup
        shell: bash
        run: |
          echo "=== Final disk status ==="
          df -h
          
          # Final cleanup of temporary files in the entire workspace
          find . -name "*.tmp" -type f -delete 2>/dev/null || true
          find . -name "*.temp" -type f -delete 2>/dev/null || true
          find . -name ".DS_Store" -type f -delete 2>/dev/null || true
          
          echo "=== After final cleanup ==="
          df -h
