name: Sync EDINET IPO reports to MKC

on:
#  schedule:
#    # ÊØéÊúà1Êó• 09:00 JSTÔºàGitHub„ÅØUTCÂü∫Ê∫ñ„Å™„ÅÆ„Åß 00:00 UTCÔºâ
#    - cron: '0 0 1 * *'
  push:
    tags:
      - 'center-*'        # ‰æã: center-2025-08-31
  workflow_dispatch:

concurrency:
  group: sync-edinet-to-mkc
  cancel-in-progress: false

jobs:
  sync:
    runs-on: ubuntu-latest
    permissions:
      contents: read   # IPOTenbaggerAnalysis „ÅÆË™≠„ÅøÂèñ„Çä„ÅÆ„Åø„ÄÇÊõ∏„ÅçËæº„Åø„ÅØPAT„ÅßÂÆüÊñΩ
    steps:
      - name: Free up disk space
        run: |
          echo "=== Before cleanup ==="
          df -h
          
          # Remove unnecessary software to free up space (more aggressive)
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf /usr/local/graalvm/
          sudo rm -rf /usr/local/.ghcup/
          sudo rm -rf /usr/local/share/powershell
          sudo rm -rf /usr/local/share/chromium
          sudo rm -rf /usr/local/lib/node_modules
          sudo rm -rf /opt/pipx_bin
          sudo rm -rf /opt/microsoft
          sudo rm -rf /usr/share/swift
          sudo rm -rf /usr/local/share/vcpkg
          sudo rm -rf /usr/local/julia*
          sudo rm -rf /usr/share/miniconda
          sudo rm -rf /usr/local/aws-cli
          
          # Clean system caches and temporary files
          sudo apt-get clean
          sudo apt-get autoremove -y
          sudo journalctl --vacuum-time=1d
          sudo find /tmp -type f -exec rm -f {} + 2>/dev/null || true
          sudo find /var/tmp -type f -exec rm -f {} + 2>/dev/null || true
          sudo find /var/cache -type f -exec rm -f {} + 2>/dev/null || true
          
          # Clear Docker if present
          docker system prune -af 2>/dev/null || true
          docker volume prune -f 2>/dev/null || true
          
          echo "=== After cleanup ==="
          df -h
      - name: Checkout IPOTenbaggerAnalysis (sparse)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          # ÂØæË±°„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ„ÅøÂèñÂæó„Åó„Å¶È´òÈÄüÂåñ
          sparse-checkout: |
            data/output/
          sparse-checkout-cone: true

      - name: Ensure secret exists
        run: |
          if [ -z "${{ secrets.MILLIONARE_KNOWLEDGE_CENTER_TOKEN }}" ]; then
            echo "::error::Secret MILLIONARE_KNOWLEDGE_CENTER_TOKEN is not set in IPOTenbaggerAnalysis (Settings > Secrets and variables > Actions)."
            exit 1
          fi

      - name: Checkout MillionareKnowledgeCenter
        uses: actions/checkout@v4
        with:
          repository: supertask/MillionareKnowledgeCenter
          ref: main                           # ÂÆõÂÖà„ÅÆ„Éá„Éï„Ç©„É´„Éà„Éñ„É©„É≥„ÉÅ„Å´Âêà„Çè„Åõ„Å¶ÂøÖË¶Å„Å™„ÇâÂ§âÊõ¥
          token: ${{ secrets.MILLIONARE_KNOWLEDGE_CENTER_TOKEN }}
          path: mkc
          fetch-depth: 0

      - name: Sync ipo_reports -> knowledge/edinet_db
        shell: bash
        run: |
          set -euo pipefail
          SRC_BASE="data/output"
          DEST_BASE="mkc/knowledge/listed_companies_data"

          if [ ! -d "${SRC_BASE}" ]; then
            echo "Source base '${SRC_BASE}' not found. Nothing to sync."
            exit 0
          fi

          # Git configuration (ÂÖ±ÈÄöË®≠ÂÆö)
          cd mkc
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          cd ..

          # Function to commit and push changes
          commit_and_push() {
            local commit_msg="$1"
            local file_count="$2"
            
            echo "=== Committing and pushing $file_count files ==="
            cd mkc
            
            # Clean up unwanted files
            find knowledge/listed_companies_data -name '.DS_Store' -type f -delete 2>/dev/null || true
            find knowledge/listed_companies_data -name '*.tmp' -type f -delete 2>/dev/null || true
            find knowledge/listed_companies_data -name '*.temp' -type f -delete 2>/dev/null || true

            git add -A
            if git diff --staged --quiet; then
              echo "No changes to commit."
              cd ..
              return 0
            fi

            git commit -m "$commit_msg"
            git push origin HEAD
            echo "‚úÖ Successfully pushed batch with $file_count files"
            cd ..
            return 0
          }

          shopt -s nullglob
          # Áõ¥‰∏ã„ÅÆ„Äå„Éï„Ç©„É´„ÉÄÁæ§„Äç„ÇíÂàóÊåôÔºà„Éï„Ç°„Ç§„É´„ÅØÁÑ°Ë¶ñÔºâ
          dirs=("${SRC_BASE}"/*/)
          if [ ${#dirs[@]} -eq 0 ]; then
            echo "No folders under ${SRC_BASE}."
            exit 0
          fi

          echo "Total directories to sync: ${#dirs[@]}"
          echo "=== Disk space before sync ==="
          df -h

          # Process directories in smaller batches to manage disk usage
          # Use very small batch size to be extremely conservative with disk space
          batch_size=10
          total_success=0
          total_failed=0
          
          # File counting for split commits
          files_in_current_batch=0
          max_files_per_push=150
          push_count=1
          
          # Disable immediate exit on error for the sync loop to handle errors gracefully
          set +e
          
          for ((i=0; i<${#dirs[@]}; i+=batch_size)); do
            batch_end=$((i + batch_size))
            if [ $batch_end -gt ${#dirs[@]} ]; then
              batch_end=${#dirs[@]}
            fi
            
            echo "=== Processing batch $((i/batch_size + 1)): directories $((i+1)) to $batch_end ==="
            
            sync_success=0
            sync_failed=0
            
            for ((j=i; j<batch_end; j++)); do
              src_dir="${dirs[j]}"
              cat_name="$(basename "${src_dir%/}")"
              dest_dir="${DEST_BASE}/${cat_name}"
              mkdir -p "${dest_dir}"

              # Count files in source directory
              dir_file_count=$(find "${src_dir}" -type f 2>/dev/null | wc -l || echo 0)
              
              echo "Syncing ${src_dir} -> ${dest_dir} ($(($j+1))/${#dirs[@]}) - ${dir_file_count} files"
              echo "  Current batch total: $((files_in_current_batch + dir_file_count)) files"
              
              # Check if adding this directory would exceed the limit
              if [ $((files_in_current_batch + dir_file_count)) -gt $max_files_per_push ] && [ $files_in_current_batch -gt 0 ]; then
                echo "  üì¶ File limit approaching ($((files_in_current_batch + dir_file_count))>${max_files_per_push}). Pushing current batch..."
                
                # Commit and push current changes
                if [[ "${GITHUB_REF:-}" == refs/tags/* ]]; then
                  TAG_NAME="${GITHUB_REF#refs/tags/}"
                  commit_msg="chore(sync): EDINET IPO reports batch ${push_count} from IPOTenbaggerAnalysis (tag ${TAG_NAME})"
                else
                  commit_msg="chore(sync): EDINET IPO reports batch ${push_count} from IPOTenbaggerAnalysis ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
                fi
                
                commit_and_push "$commit_msg" "$files_in_current_batch"
                
                # Reset counters
                files_in_current_batch=0
                ((push_count++))
              fi
              
              # ÊúÄÈÅ©Âåñ„Åï„Çå„Åürsync„Ç™„Éó„Ç∑„Éß„É≥ÔºöÂúßÁ∏Æ„ÄÅÈÄ≤ÊçóË°®Á§∫„ÄÅÂäπÁéáÁöÑ„Å™Ëª¢ÈÄÅ
              echo "  About to execute rsync command..."
              rsync_exit_code=0
              rsync -avz --compress-level=6 --partial --inplace \
                --delete --delete-excluded \
                --exclude='.git/' \
                --exclude='.DS_Store' \
                --exclude='*.tmp' \
                --exclude='*.temp' \
                --timeout=300 \
                "${src_dir}/" "${dest_dir}/" || rsync_exit_code=$?
              
              echo "  rsync completed with exit code: ${rsync_exit_code}"
              
              if [ "${rsync_exit_code}" -eq 0 ]; then
                echo "‚úì Successfully synced ${src_dir}"
                echo "  Destination contents: $(ls -la "${dest_dir}" 2>/dev/null | wc -l) items"
                ((sync_success++))
                # Add to file count only if sync was successful
                ((files_in_current_batch += dir_file_count))
              else
                echo "‚úó Failed to sync ${src_dir} (error code: ${rsync_exit_code})"
                ((sync_failed++))
                
                              # If disk space is the issue, try to free up space and continue
              available_space=$(df . | tail -1 | awk '{print $4}')
              if [ "$available_space" -lt 1000000 ]; then  # Less than 1GB
                echo "Critical: Low disk space. Attempting emergency cleanup..."
                sudo apt-get clean
                sudo apt-get autoremove -y
                docker system prune -af 2>/dev/null || true
                docker volume prune -f 2>/dev/null || true
                find /tmp -type f -delete 2>/dev/null || true
                find /var/tmp -type f -delete 2>/dev/null || true
                find /var/cache -type f -delete 2>/dev/null || true
                sudo journalctl --vacuum-time=1d
                
                # Force memory cleanup
                sudo sysctl -w vm.drop_caches=3
                
                # Check if we have enough space to continue
                available_space=$(df . | tail -1 | awk '{print $4}')
                if [ "$available_space" -lt 800000 ]; then  # Less than 800MB
                  echo "Error: Still insufficient disk space after cleanup. Cannot continue."
                  exit 1
                fi
              fi
              fi
              
              echo "  Completed processing directory $(($j+1))/${#dirs[@]}: ${cat_name} (batch total: ${files_in_current_batch} files)"
              
              # Clean up source directory after successful sync to save space
              if [ "${rsync_exit_code}" -eq 0 ] && [ -d "${src_dir}" ]; then
                echo "  Cleaning up source directory to save disk space..."
                rm -rf "${src_dir}" 2>/dev/null || true
                echo "  Source directory cleaned: ${src_dir}"
              fi
            done
            
            echo "Batch $((i/batch_size + 1)) summary: ${sync_success} successful, ${sync_failed} failed"
            
            # Update total counters
            ((total_success += sync_success))
            ((total_failed += sync_failed))
            
            # Check disk usage after each batch and clean up if needed
            echo "=== Disk usage after batch $((i/batch_size + 1)) ==="
            df -h
            
            # Clean up temporary files
            find . -name "*.tmp" -type f -delete 2>/dev/null || true
            find . -name "*.temp" -type f -delete 2>/dev/null || true
            
            # More aggressive disk monitoring and cleanup
            available_space=$(df . | tail -1 | awk '{print $4}')
            if [ "$available_space" -lt 2000000 ]; then  # Less than 2GB available
              echo "Warning: Low disk space ($available_space KB available). Running proactive cleanup..."
              sudo apt-get clean
              sudo apt-get autoremove -y
              docker system prune -af 2>/dev/null || true
              docker volume prune -f 2>/dev/null || true
              find /tmp -type f -delete 2>/dev/null || true
              find /var/tmp -type f -delete 2>/dev/null || true
              sudo journalctl --vacuum-time=1d
              sudo sysctl -w vm.drop_caches=3
              echo "Cleanup completed. New available space: $(df . | tail -1 | awk '{print $4}') KB"
            fi
          done
          
          # Re-enable immediate exit on error after sync loop
          set -e
          
          # Push any remaining files
          if [ $files_in_current_batch -gt 0 ]; then
            echo "=== Pushing final batch with ${files_in_current_batch} remaining files ==="
            if [[ "${GITHUB_REF:-}" == refs/tags/* ]]; then
              TAG_NAME="${GITHUB_REF#refs/tags/}"
              commit_msg="chore(sync): EDINET IPO reports batch ${push_count} (final) from IPOTenbaggerAnalysis (tag ${TAG_NAME})"
            else
              commit_msg="chore(sync): EDINET IPO reports batch ${push_count} (final) from IPOTenbaggerAnalysis ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
            fi
            
            commit_and_push "$commit_msg" "$files_in_current_batch"
          fi

          # Final sync statistics
          echo "=== SYNC COMPLETE ==="
          echo "Total directories processed: ${#dirs[@]}"
          echo "Successfully synced: ${total_success}"
          echo "Failed to sync: ${total_failed}"
          echo "Total push batches: ${push_count}"
          
          if [ "$total_failed" -gt 0 ]; then
            echo "‚ö†Ô∏è  Warning: Some directories failed to sync. Check logs above for details."
            if [ "$total_failed" -gt "$((${#dirs[@]} / 2))" ]; then
              echo "‚ùå Error: More than half of the directories failed. Exiting with error."
              exit 1
            fi
          else
            echo "‚úÖ All directories synced successfully!"
          fi

      - name: Final cleanup
        shell: bash
        run: |
          echo "=== Final disk status ==="
          df -h
          
          # Final cleanup of temporary files in the entire workspace
          find . -name "*.tmp" -type f -delete 2>/dev/null || true
          find . -name "*.temp" -type f -delete 2>/dev/null || true
          find . -name ".DS_Store" -type f -delete 2>/dev/null || true
          
          echo "=== After final cleanup ==="
          df -h
